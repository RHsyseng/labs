[{"content":"","date":-62135596800,"description":"","objectID":"fd9b822ece87dc0d68cfb2d19d1f7ac2","permalink":"https://pages.sysdeseng.com/labs/deploy/baremetalhost/","title":"BareMetalHost"},{"content":"Reference Deploy Folder: https://github.com/RHsyseng/labs-index/tree/master/deploy-BMHost-virtual-over-acm\nTo have a full reference there\u0026amp;rsquo;s a deploy script inside the deploy folder mentioned before, with all the steps shown here.\nScenario Description The main goal of this document is to explain the deployment of a baremetal host over RHACM. This lab uses sushy tools to emulate the same situation which we could found using a real Baremetal host.\nRequirements  Hub cluster deployed successfully LocalStorage ready Hive operator ready ACM with AI Operator ready  Install sushy-tools \u0026amp;nbsp;Information If you\u0026amp;rsquo;ve installed IPI baremetal with Kcli and virtual_protocol: Redfish -\u0026amp;gt; you could omit this step because it is already installed.   This will create a service running in the port :8000 which will be the emulation for the Redfish protocol\ndnf -y install pkgconf-pkg-config libvirt-devel gcc python3-libvirt python3 git pip3 install sushy-tools IP=$(ip -o addr show eth0 | head -1 | awk \u0026amp;#39;{print $4}\u0026amp;#39; | cut -d \u0026amp;#34;/\u0026amp;#34; -f 1 | head -1) sed -i \u0026amp;#34;s/127.0.0.1/$IP/\u0026amp;#34; /etc/sushy.conf # cp /root/bin/sushy.service /usr/lib/systemd/system # restorecon -Frv /usr/lib/systemd/system/sushy.service systemctl enable --now sushy sleep 20 ssh-keyscan -H {{ config_host }} \u0026amp;gt;\u0026amp;gt; /root/.ssh/known_hosts echo -e \u0026amp;#34;Host=*\\nStrictHostKeyChecking=no\\n\u0026amp;#34; \u0026amp;gt; /root/.ssh/config python3 /root/bin/sushy.py At this point, make sure you have the next option by default:\nSUSHY_EMULATOR_IGNORE_BOOT_DEVICE=True Create the VM (host) Create the image with uefi=true and start=false because we’re gonna use the sushy tools. If you\u0026amp;rsquo;re using a SNO only have to use one of them.\nkcli --debug create vm -P uefi_legacy=true -P start=false -P nets=[\u0026amp;#39;{\u0026amp;#34;name\u0026amp;#34;:\u0026amp;#34;bare-net\u0026amp;#34;, \u0026amp;#34;mac\u0026amp;#34;:\u0026amp;#34;ee:bb:aa:ee:1e:1a\u0026amp;#34;}\u0026amp;#39;] -P memory=38000 -P numcpus=8 -P disks=[180] agent1 kcli --debug create vm -P uefi_legacy=true -P start=false -P …","date":-62135596800,"description":"","objectID":"4b82930b1796d6dce49fbb5dbff0411e","permalink":"https://pages.sysdeseng.com/labs/deploy/baremetalhost/virtual-over-acm/","tags":["ACM","labs","Virtual Baremetal"],"title":"Virtual over ACM"},{"content":"Reference Deploy Folder: https://github.com/RHsyseng/labs-index/tree/master/deploy-BMHost-virtual-over-AI\nTo have a full reference there\u0026amp;rsquo;s a deploy script inside the deploy folder mentioned before, with all the steps shown here.\nScenario Description The main goal of this document is to explain the deployment of a baremetal host over Assisted Installer. This lab uses sushy-tools to emulate the same situation which we could found using a real Baremetal host.\nRequirements  Hub cluster deployed successfully LocalStorage ready Hive operator ready Assisted Installer Operator ready  Install sushy-tools \u0026amp;nbsp;Information If you\u0026amp;rsquo;ve installed IPI baremetal with Kcli and virtual_protocol: Redfish -\u0026amp;gt; you could omit this step because it is already installed.   This will create a service running in the port :8000 which will be the emulation for the Redfish protocol\ndnf -y install pkgconf-pkg-config libvirt-devel gcc python3-libvirt python3 git pip3 install sushy-tools IP=$(ip -o addr show eth0 | head -1 | awk \u0026amp;#39;{print $4}\u0026amp;#39; | cut -d \u0026amp;#34;/\u0026amp;#34; -f 1 | head -1) sed -i \u0026amp;#34;s/127.0.0.1/$IP/\u0026amp;#34; /etc/sushy.conf # cp /root/bin/sushy.service /usr/lib/systemd/system # restorecon -Frv /usr/lib/systemd/system/sushy.service systemctl enable --now sushy sleep 20 ssh-keyscan -H {{ config_host }} \u0026amp;gt;\u0026amp;gt; /root/.ssh/known_hosts echo -e \u0026amp;#34;Host=*\\nStrictHostKeyChecking=no\\n\u0026amp;#34; \u0026amp;gt; /root/.ssh/config python3 /root/bin/sushy.py At this point make sure you have the next option by default:\nSUSHY_EMULATOR_IGNORE_BOOT_DEVICE = True Create the VM (host) Create the image with uefi=true and start=false because we\u0026amp;rsquo;re gonna use the sushy tools. If you\u0026amp;rsquo;re using a SNO only have to use one of them.\nkcli --debug create vm -P uefi_legacy=true -P start=false -P nets=[\u0026amp;#39;{\u0026amp;#34;name\u0026amp;#34;:\u0026amp;#34;bare-net\u0026amp;#34;, \u0026amp;#34;mac\u0026amp;#34;:\u0026amp;#34;ee:bb:aa:ee:1e:1a\u0026amp;#34;}\u0026amp;#39;] -P memory=38000 -P numcpus=8 -P disks=[180] agent1 kcli --debug create vm -P uefi_legacy=true -P start=false -P …","date":-62135596800,"description":"","objectID":"9d3b5c7df2d0747b4ea752f4f7c86307","permalink":"https://pages.sysdeseng.com/labs/deploy/baremetalhost/virtual-over-ai/","tags":["Assisted Installer","labs","Virtual Baremetal"],"title":"Virtual over Assisted Installer"},{"content":"Reference Deploy Folder: https://github.com/RHsyseng/labs-index/tree/master/deploy-hub-spoke-over-acm\nTo have a full reference there\u0026amp;rsquo;s a deploy script inside the deploy folder mentioned before, with all the steps shown here.\nCreate the pull-secret This pull-secret will be injected into the discovery ISO, so it needs to be a valid pull-secret and be able to pull from the registries you\u0026amp;rsquo;re going to use for the deployment. If you have some doubts you could use the next tool to verify it: pull secret validator\napiVersion: v1 kind: Secret metadata: name: assisted-deployment-pull-secret namespace: open-cluster-management stringData: .dockerconfigjson: \u0026amp;#34;PULL_SECRET\u0026amp;#34; type: kubernetes.io/dockerconfigjson SSH private key This SSH key will be used for the user core, the one we will be able to use to ssh into our CoreOS-deployed systems.\napiVersion: v1 kind: Secret metadata: creationTimestamp: null name: assisted-deployment-ssh-private-key namespace: open-cluster-management type: Opaque stringData: --redacted-- ClusterImageSet This is the reference we\u0026amp;rsquo;ve used in the AI cluster deployment. So, you have to use the same name, and also replace the tag with the OCP release you\u0026amp;rsquo;re using. If you want to query available image release look at this here\napiVersion: hive.openshift.io/v1 kind: ClusterImageSet metadata: name: openshift-v4.8.0 namespace: open-cluster-management spec: releaseImage: TAG_OCP_IMAGE_RELEASE Config map for the assisted service config This is the way to pass some configuration to the assisted service. In our case just log level set to debug\napiVersion: v1 kind: ConfigMap metadata: name: assisted-service-config namespace: open-cluster-management labels: app: assisted-service data: LOG_LEVEL: \u0026amp;#34;debug\u0026amp;#34; Agent Service Config This file should contain the osImages which will be used.\n\u0026amp;nbsp;Important Here if you\u0026amp;rsquo;re using a Connected environment could be directly the mirror. But, If you\u0026amp;rsquo;re using a disconnected environment, …","date":-62135596800,"description":"","objectID":"56e32272031556741743669e7e5959c5","permalink":"https://pages.sysdeseng.com/labs/deploy/hub-spoke/acm/","tags":["ACM","labs","Spoke"],"title":"Using Advanced Cluster Management (ACM)"},{"content":"Reference Deploy Folder: https://github.com/RHsyseng/labs-index/tree/master/deploy-hub-spoke-over-AI To have a full reference there\u0026amp;rsquo;s a deploy script inside the deploy folder mentioned before, with all the steps shown here.\nCreate the pull-secret This pull-secret will be injected into the discovery ISO, so it needs to be a valid pull-secret and be able to pull from the registries you\u0026amp;rsquo;re going to use. If you have some doubts you could use the next tool to verify it: pull secret validator\napiVersion: v1 kind: Secret metadata: name: assisted-deployment-pull-secret namespace: assisted-installer stringData: .dockerconfigjson: \u0026amp;#34;... \u0026amp;lt;redacted\u0026amp;gt; ...\u0026amp;#34; type: kubernetes.io/dockerconfigjson Create the SSH private key This SSH key will be used for the user core.\napiVersion: v1 kind: Secret metadata: creationTimestamp: null name: assisted-deployment-ssh-private-key namespace: assisted-installer type: Opaque stringData: Cluster Image Set This is the reference we\u0026amp;rsquo;ve used in the AI cluster deployment. So, you have to use the same name, and also replace the tag with the OCP release you\u0026amp;rsquo;re using. If you want to query available image release look at this here\napiVersion: hive.openshift.io/v1 kind: ClusterImageSet metadata: name: openshift-v4.8.0 spec: releaseImage: TAG_OCP_IMAGE_RELEASE Create the cluster deployment with the new spec The important thing here is the \u0026amp;ldquo;cluster installer reference\u0026amp;rdquo; just to point to the new configuration \u0026amp;ldquo;agent Cluster Install\u0026amp;rdquo; which will be defined in the next step.\napiVersion: hive.openshift.io/v1 kind: ClusterDeployment metadata: name: lab-cluster namespace: assisted-installer spec: baseDomain: alklabs.com clusterInstallRef: group: extensions.hive.openshift.io kind: AgentClusterInstall name: lab-cluster-aci version: v1beta1 clusterName: lab-spoke controlPlaneConfig: servingCertificates: {} platform: agentBareMetal: agentSelector: matchLabels: bla: aaa pullSecretRef: name: …","date":-62135596800,"description":"","objectID":"723d4fe8f0722e242a5cf42e44c1d882","permalink":"https://pages.sysdeseng.com/labs/deploy/hub-spoke/ai/","tags":["Assisted Installer","labs","Spoke","AI"],"title":"Using Assisted Installer (AI)"},{"content":"","date":-62135596800,"description":"","objectID":"7a5c5e76e770969313d610eac6979c73","permalink":"https://pages.sysdeseng.com/labs/deploy/hub-spoke/","title":"Hub \u0026 Spoke cluster"},{"content":"Reference  Deploy Folder: https://github.com/RHsyseng/labs-index/tree/master/deploy-AssistedInstaller-operator External Operator ref: https://github.com/openshift/assisted-service quay.io reference tag: https://quay.io/repository/ocpmetal/assisted-service-index?tab=tags  Assisted Installer Operator Catalog Source First thing is to create the catalog source with the quay image from the Development team.\nThe only thing we need to change is the tag for the image using the reference url\n--- apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: assisted-service namespace: openshift-marketplace spec: image: quay.io/ocpmetal/assisted-service-index:\u0026amp;lt;tag from quay.io\u0026amp;gt; sourceType: grpc  In case you want to go with the version of the Operator Hub, in the extra folder you\u0026amp;rsquo;ve got the file assisted-installer-CS-operatorHub.yml In the case you want to compile your own image, you could follow the steps in the extra folder script: create-quay-image.sh  Requirements for Assisted Installer The idea is to create the namespace and fulfill the remaining requirements before installing the operator:\n--- apiVersion: v1 kind: Namespace metadata: name: assisted-installer labels: name: assisted-installer AI Operator Group --- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: assisted-installer-operator namespace: assisted-installer spec: targetNamespaces: - assisted-installer Subscription AI Now it\u0026amp;rsquo;s time for the subscription, but there are important things to keep in mind:\n OpenShift version that you have to force deploying the 4.8 Deploy_target: onprem if you\u0026amp;rsquo;re on BareMetalHost  --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: assisted-service-operator namespace: assisted-installer spec: channel: alpha installPlanApproval: Automatic name: assisted-service-operator source: assisted-service sourceNamespace: openshift-marketplace At this point, now this should be fixed, but we will keep it …","date":-62135596800,"description":"","objectID":"90cb273d7ef7575ae5470e5f80e0c463","permalink":"https://pages.sysdeseng.com/labs/deploy/operator/assistedinstaller/","tags":["assisted-installer","AI","operator","labs"],"title":"Assisted Installer"},{"content":"Reference Deploy Folder: https://github.com/RHsyseng/labs-index/tree/master/deploy-hive-operator\nHive Operator --- apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: global-operators namespace: openshift-operators spec: targetNamespaces: - openshift-operators --- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: hive-operator namespace: openshift-operators spec: channel: alpha installPlanApproval: Automatic name: hive-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: hive-operator.v1.1.2 ","date":-62135596800,"description":"","objectID":"80715dba89ddbed157b5f833dec9bcfb","permalink":"https://pages.sysdeseng.com/labs/deploy/operator/hive/","tags":["operator","Hive","labs"],"title":"Hive"},{"content":"Reference Deploy Folder: https://github.com/RHsyseng/labs-index/tree/master/deploy-LocalStorage-operator\nDeploy 5 disks for each nodes # kcli create disk -s 10 qct11-master-x ---scripted for node in `seq 0 2`; do for a in `seq 0 5`; do kcli create disk -s 10 qct11-master-$node; done; done Create the local storage operator  namespace operator group subscription  1-. Namespace: To do that just oc apply -f 01-LS-namespace.yml:\n--- apiVersion: v1 kind: Namespace metadata: name: openshift-local-storage 2-. Operator group:\n--- apiVersion: operators.coreos.com/v1alpha2 kind: OperatorGroup metadata: name: local-operator-group namespace: openshift-local-storage spec: targetNamespaces: - openshift-local-storage 3-. Catalog source:\n--- apiVersion: operators.coreos.com/v1alpha1 kind: CatalogSource metadata: name: localstorage-operator-manifests namespace: openshift-local-storage spec: sourceType: grpc image: quay.io/gnufied/gnufied-index:1.0.0 4-. Subscription:\n--- apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: local-storage-subscription namespace: openshift-local-storage spec: channel: preview # this is the default channel name defined in opm-bundle file name: local-storage-operator source: localstorage-operator-manifests sourceNamespace: openshift-local-storage 5-. Local Volume:\napiVersion: local.storage.openshift.io/v1 kind: LocalVolume metadata: name: fs namespace: openshift-local-storage spec: logLevel: Normal managementState: Managed storageClassDevices: - devicePaths: - /dev/vdc - /dev/vdd - /dev/vde - /dev/vdf - /dev/vdg fsType: ext4 storageClassName: fs-lso volumeMode: Filesystem 6-. Patch the LocalStorage Kubernetes:\nkubectl patch storageclass fs-lso -p \u0026amp;#39;{\u0026amp;#34;metadata\u0026amp;#34;: {\u0026amp;#34;annotations\u0026amp;#34;:{\u0026amp;#34;storageclass.kubernetes.io/is-default-class\u0026amp;#34;:\u0026amp;#34;true\u0026amp;#34;}}}\u0026amp;#39; ","date":-62135596800,"description":"","objectID":"a173191b42df9a2f6d8589e04b65cf2f","permalink":"https://pages.sysdeseng.com/labs/deploy/operator/localstorage/","tags":["Operator","LocalStorage","labs"],"title":"LocalStorage"},{"content":"","date":-62135596800,"description":"","objectID":"1cfc70e57791ede03c7df5a8fde91185","permalink":"https://pages.sysdeseng.com/labs/deploy/operator/","title":"Operator"},{"content":"","date":-62135596800,"description":"","objectID":"caf820e17d553017cf3b3b36853c70d4","permalink":"https://pages.sysdeseng.com/labs/deploy/remoteworkernode/","title":"RemoteWorkerNode"},{"content":"Requirements The starting point of this document is based on the following things installed previously:\n Hub clusters ocp ipi baremetal installed (multinode) spoke cluster multinode installed. You could use the deploy.sh script to accomplish this.  Day2 workflow to add worker nodes 1-. Deploy hub cluster (Requirements)\n2-. Add bmh crs to hub clusters (in order to deploy spoke cluster multi node) (Requirements)\n3-. Finish the spoke cluster installation (multinode 3 nodes)\n4-. Add new manifest to create a new bmh on hub cluster (in this case with the worker role)\n5-. Automatically cluster will be cluster-day2 and the worker will be added to the spoke cluster as a worker.\n6-. Download kubeconfig for spoke-cluster\n7-. Validate oc get nodes to have 3 nodes master and the new worker node\nConsiderations to the lab   During the lab setup we\u0026amp;rsquo;ve found the next bug to use the hostname created with the annotation: https://github.com/openshift/assisted-service/pull/2586\n  After solved with a local image to validate the PR fix the problem, everything is working fine.\n  Deployment used in lab-index: https://github.com/RHsyseng/labs-index/actions/runs/1221173459\n  Verify the multinode spoke cluster installation Once the spoke cluster is installed, you can verify the installation by running the following command:\n[root@qct-d14u11 ~]# oc get nodes NAME STATUS ROLES AGE VERSION test-ci-master-0 Ready master,virtual,worker 3h22m v1.21.1+f36aa36 test-ci-master-1 Ready master,virtual,worker 3h23m v1.21.1+f36aa36 test-ci-master-2 Ready master,virtual,worker 3h23m v1.21.1+f36aa36 [root@qct-d14u11 ~]# oc get bmh NAME STATE CONSUMER ONLINE ERROR lab-agent1 provisioned true lab-agent2 provisioned true lab-agent3 provisioned true [root@qct-d14u11 ~]# oc get agent NAME CLUSTER APPROVED ROLE STAGE 0211833c-5a24-4ee0-bfd7-7d866cb9da56 lab-cluster true master Done 33ec7440-9e9c-46a1-896d-e5a13e1553cd lab-cluster true master Done 96b2009e-f3eb-458c-862b-2d1dac1e2e08 lab-cluster true master …","date":-62135596800,"description":"","objectID":"e05036a16c51779bd11fcc7f5e8e41cc","permalink":"https://pages.sysdeseng.com/labs/deploy/remoteworkernode/day2-spoke-worker/","tags":["RWN","Assisted Installer","day2","labs"],"title":"Day2 with spoke cluster to add worker nodes"},{"content":"Requirements The starting point of this document is based on the following things installed previously:\n Hub clusters ocp ipi baremetal installed (multinode) Hive operator installed AI installed  Create the pull secret You need to create a secret in the cluster that contains the pull secret for the registry.\napiVersion: v1 kind: Secret metadata: name: assisted-deployment-pull-secret namespace: assisted-installer stringData: .dockerconfigjson: \u0026amp;#39;PULL_SECRET\u0026amp;#39; # replace with your pull secret type: kubernetes.io/dockerconfigjson Create the private ssh key You need to create a secret in the cluster that contains the private ssh key for the registry.\napiVersion: v1 kind: Secret metadata: creationTimestamp: null name: assisted-deployment-ssh-private-key namespace: assisted-installer type: Opaque stringData: # paste your private ssh key here Create the Infraenv without cluster reference The idea is create the infraenv without cluster reference in order to keep the agent alone (without binding to a cluster). This is a good use case in order to keep nodes prepared to be added to a cluster later. To do that, you need to remove the reference from this manifest to get something like this:\napiVersion: agent-install.openshift.io/v1beta1 kind: InfraEnv metadata: name: lab-env namespace: assisted-installer spec: sshAuthorizedKey: PUB_SSH_KEY agentLabelSelector: matchLabels: bla: aaa pullSecretRef: name: assisted-deployment-pull-secret ignitionConfigOverride: \u0026amp;#34;\u0026amp;#34; Verify the infraenv URL without cluster reference As you can see in the output, the URL is present, but the cluster reference does not appear. So, when the agent boot up will be ready with the image but not bound to a cluster.\napiVersion: v1 items: - apiVersion: agent-install.openshift.io/v1beta1 kind: InfraEnv metadata: creationTimestamp: \u0026amp;#34;2021-09-29T09:41:49Z\u0026amp;#34; generation: 1 managedFields: - apiVersion: agent-install.openshift.io/v1beta1 fieldsType: FieldsV1 fieldsV1: f:status: .: {} …","date":-62135596800,"description":"","objectID":"9ab532a6fab403826a74f99b57930836","permalink":"https://pages.sysdeseng.com/labs/deploy/remoteworkernode/late-binding/","tags":["late binding","Assisted Installer","agents","labs"],"title":"Late Binding adding new agents"},{"content":"Reference Deploy Folder: https://github.com/RHsyseng/labs-index/tree/master/deploy-rwn-to-ai-using-aicli-api\nScenario Description Once we\u0026amp;rsquo;ve got a 3 master cluster installed with the AI operator, this document is useful to add new remote worker nodes (maybe worker nodes using the same network)\nIn this case, we\u0026amp;rsquo;ve got 2 possibilities to do that:\n Using the API or maybe an abstraction using AICLI Using the CRD operator (not developed yet)  Requirement We need an environment with:\n Hub deployment ready (UPI,IPI whatever) AI operator + Hive + LocalStorage Cluster 3 multi-node (not possible to do using SNO). The word single node is just to have one node without any possibility to add workers  [root@qct-d14u11 rwn-ai-crd]# oc get agents NAME CLUSTER APPROVED 2e37b5e4-2295-43a2-a1a1-3383271fb013 cluster-crd-rwn true 50ced8d3-d1d3-43ad-8dcc-df0239217707 cluster-crd-rwn true ae12effa-47b9-433e-9b0e-20041092aa9d cluster-crd-rwn true Now, we\u0026amp;rsquo;re going to deploy a remote worker node but in a different network L3 segment: assisted-net -\u0026amp;gt; 192.168.7.0/24\nCreate new cluster *-day2 using API/library: The idea is to create a new cluster adding a suffix “-day2” in order to add the remote worker node to this cluster. The idea is basically join or create a trick to associate the new cluster -day2 to the original to add new hosts but not directly using the original cluster. So the steps should be:\n Create a new cluster -day2 Create the discovery ISO associated to this new cluster -day2 Spin up a new VM (remote worker) using a different L3 network (assisted-net) booting from this Discovery-day2 image Change the information required (hostname, api_vip, and so on) FIX the L2 Support Environment changing the config map  Using AICLI: export AI_URL=http://\u0026amp;lt;url-api\u0026amp;gt;:\u0026amp;lt;port\u0026amp;gt; aicli create cluster cluster-rwn-alb2-day2 To verify day2 you could do the next command to view \u0026amp;ldquo;adding-host\u0026amp;rdquo; state:\n[root@qct-d14u11 ~]# aicli list cluster …","date":-62135596800,"description":"","objectID":"b240e813aad456e9d2d243cd4adc5d2e","permalink":"https://pages.sysdeseng.com/labs/deploy/remoteworkernode/rwn-to-ai-using-aicli-api/","tags":["RWN","Assisted Installer","AICLI","labs"],"title":"RWN to Assisted Installer using AICLI API"},{"content":"Requirements The starting point of this document is based on the following things installed previously:\n Hub clusters ocp ipi baremetal installed (multinode) spoke cluster multinode installed. You could use the deploy.sh script to accomplish this.  Remote worker node into Day2 spoke cluster workflow 1-. Deploy hub cluster (Requirement)\n2-. Add bmh crs to hub clusters (in order to deploy spoke cluster multi node) (Requirement)\n3-. Finish the spoke cluster installation (multinode 3 nodes)\n4-. Create the new VM into a different L3 network\n5-. Add new manifest to create a new bmh on hub cluster (in this case with the worker role)\n6-. Automatically cluster will be cluster-day2 and the worker will be added to the spoke cluster as a worker.\n7-. Download kubeconfig for spoke-cluster\n8-. Validate oc get nodes to have 3 nodes master and the new worker node in a different L3 network\nConsiderations to the lab environment   During the lab setup we\u0026amp;rsquo;ve found the next bug to use the hostname created with the annotation: PR hostname\n  After solved with a local image to validate the PR fix the problem, everything is working fine with latest.\n  Verified the pull request for RWN: PR RWN\n  With Libvirt and firewalld is necessary modify firewalld (stop firewalld or maybe add new rules to communicate both L3 networks)\n  Deployment used in lab-index: https://github.com/RHsyseng/labs-index/actions/runs/1252805471\n  In this case the vm will be in a different L3 network. Just to make easy:\n Network Bare-net: 192.168.150.0/24 In this network will be Hub and Spoke cluster nodes/vm Network Default: 192.168.122.0/24 In this network will be the remote worker node    Verify the multinode spoke cluster installation Once the spoke cluster is installed, you can verify the installation by running the following command:\n[root@qct-d14u11 ~]# oc get nodes NAME STATUS ROLES AGE VERSION test-ci-master-0 Ready master,virtual,worker 3h22m v1.21.1+f36aa36 test-ci-master-1 Ready master,virtual,worker …","date":-62135596800,"description":"","objectID":"4af9eeace78cc7048692e765c4c2a4ca","permalink":"https://pages.sysdeseng.com/labs/deploy/remoteworkernode/rwn-to-day2-spoke-worker/","tags":["RWN","Assisted Installer","day2","labs"],"title":"Remote Worker Node in Day2 spoke cluster to add worker nodes"},{"content":"Requirements The starting point of this document is based on the following things installed previously:\n Hub clusters ocp ipi baremetal installed (multinode) spoke cluster multinode installed. You could use the deploy.sh script to accomplish this.  Day2 workflow to add worker nodes 1-. Deploy hub cluster (Requirements)\n2-. Add bmh crs to hub clusters (in order to deploy spoke cluster multi node) (Requirements)\n3-. Finish the spoke cluster installation (multinode 3 nodes)\n4-. Add new manifest to create a new bmh on hub cluster (in this case with the worker role)\n5-. Automatically cluster will be cluster-day2 and the worker will be added to the spoke cluster as a worker.\n6-. Download kubeconfig for spoke-cluster\n7-. Validate oc get nodes to have 3 nodes master and the new worker node\nConsiderations to the lab   During the lab setup we\u0026amp;rsquo;ve found the next bug to use the hostname created with the annotation: https://github.com/openshift/assisted-service/pull/2586\n  After solved with a local image to validate the PR fix the problem, everything is working fine.\n  Deployment used in lab-index: https://github.com/RHsyseng/labs-index/actions/runs/1221173459\n  Verify the multinode spoke cluster installation Once the spoke cluster is installed, you can verify the installation by running the following command:\n[root@qct-d14u11 ~]# oc get nodes NAME STATUS ROLES AGE VERSION test-ci-master-0 Ready master,virtual,worker 3h22m v1.21.1+f36aa36 test-ci-master-1 Ready master,virtual,worker 3h23m v1.21.1+f36aa36 test-ci-master-2 Ready master,virtual,worker 3h23m v1.21.1+f36aa36 [root@qct-d14u11 ~]# oc get bmh NAME STATE CONSUMER ONLINE ERROR lab-agent1 provisioned true lab-agent2 provisioned true lab-agent3 provisioned true [root@qct-d14u11 ~]# oc get agent NAME CLUSTER APPROVED ROLE STAGE 0211833c-5a24-4ee0-bfd7-7d866cb9da56 lab-cluster true master Done 33ec7440-9e9c-46a1-896d-e5a13e1553cd lab-cluster true master Done 96b2009e-f3eb-458c-862b-2d1dac1e2e08 lab-cluster true master …","date":-62135596800,"description":"","objectID":"b99721aaf0c13055c5987dd9a5693053","permalink":"https://pages.sysdeseng.com/labs/deploy/remoteworkernode/rwn-using-ai-crd/","tags":["RWN","Assisted Installer","day2","labs"],"title":"Day2 with spoke cluster to add worker nodes"},{"content":"       async function startLunrJSAsync() { console.log(\u0026#34;search: Starting Lunr...\u0026#34;); let ok = false; const lunrIndex = \u0026#34;https:\\/\\/pages.sysdeseng.com\\/labs/algolia.json\u0026#34;; const lunrSummary = \u0026#34;https:\\/\\/pages.sysdeseng.com\\/labs/algolia.json\u0026#34;; console.log(\u0026#34;search: Fetching Index...\u0026#34;); let response = await fetch(lunrIndex); let data = await response.json(); var idx = lunr(function () { this.field(\u0026#39;id\u0026#39;); this.field(\u0026#39;title\u0026#39;, { boost: 3 }); this.field(\u0026#39;date\u0026#39;); this.field(\u0026#39;content\u0026#39;); this.field(\u0026#39;permalink\u0026#39;); for (var key in data) { this.add({ \u0026#39;id\u0026#39;: key, \u0026#39;title\u0026#39;: data[key].title, \u0026#39;date\u0026#39;: data[key].date, \u0026#39;content\u0026#39;: data[key].content, \u0026#39;permalink\u0026#39;: data[key].permalink, }); } }); pages = data console.log(\u0026#34;search: Index Loaded!\u0026#34;); console.log(\u0026#34;search: Lunr Is Ready!\u0026#34;); ok = true; let obj = { idx: idx, pages: pages, ok: ok }; return obj; } function searchSite(search, query) { let template = document.querySelector(\u0026#34;#search-item\u0026#34;); let resultsContainer = document.querySelector(\u0026#34;#search-results\u0026#34;); resultsContainer.innerHTML = \u0026#34;\u0026#34;; let allResults = search.idx.search(query); if (allResults.length === 0) resultsContainer.innerHTML = \u0026#34;Nothing found; search for something else!\n\u0026#34;; else allResults.forEach(function (result) { let output = document.importNode(template.content, true); let title = output.querySelector(\u0026#34;a\u0026#34;); let breadcrumb = output.querySelector(\u0026#34;aside\u0026#34;); let summary = output.querySelector(\u0026#34;p\u0026#34;); let docRef, typemoji; title.innerHTML = pages[result.ref].title; title.setAttribute(\u0026#34;href\u0026#34;, pages[result.ref].permalink); breadcrumb.innerHTML = pages[result.ref].title; resultsContainer.appendChild(output); }); } (async () = { let Search = await startLunrJSAsync(); let query; let params = new URLSearchParams(document.location.search.substring(1)); if (params.get(\u0026#34;q\u0026#34;)) { document.getElementById(\u0026#34;search-input\u0026#34;).value = params.get(\u0026#34;q\u0026#34;); query = params.get(\u0026#34;q\u0026#34;); searchSite(Search, query); } })();   Search site      \n   Disclaimer \u0026amp;nbsp;UNSUPPORTED This repository and its contents are …","date":-62135596800,"description":"","objectID":"a2e764ae18c853be137e68789ddd3f31","permalink":"https://pages.sysdeseng.com/labs/deploy/","title":"Deploy Operators and Resources"},{"content":"In this laboratory, we will deploy over IPv6 using a disconnected environment, that means, having no Internet connectivity to reach the external servers required for installation.\nHosts hostname: bm-cluster-1-hyper.e2e.bos.redhat.com\n\u0026amp;nbsp;danger Must use kni user instead of root, so su - kni   References Lab Folder: N/A (There isn\u0026amp;rsquo;t any Lab folder in this repository because this document is just the documentation for the Disconnected Physical Lab.)\nLab info https://gitlab.cee.redhat.com/sysdeseng/pit-hybrid/-/blob/master/docs/acm-verizon-demo-ipv6/baremetal-ipv6.md\nOLM operator info: https://linuxera.org/integrating-operators-olm/ https://github.com/operator-framework/operator-registry/blob/v1.12.6/docs/design/operator-bundle.md\nArchitecture and description https://gitlab.cee.redhat.com/sysdeseng/pit-hybrid/-/blob/master/docs/acm-verizon-demo-ipv6/baremetal-ipv6.md\nClusters Configuration  1 Helper node acting as DNS/DHCP/Registry for all clusters, it\u0026amp;rsquo;s reachable over IPv6 All nodes have two NICs configured:  1 NIC - 10GB for Provision traffic (eno1)  A bridge named Provisioning is configured in the OS We do VLAN Tagging (id 3790 and 3791) in the OS, this NIC is connected to a switch port configured as hybrid mode   1 NIC - 1GB for Baremetal traffic (enp3s0f0)  A bridge named Baremetal is configured in the OS We do VLAN Tagging (id 152 and 153) in the OS, this NIC is connected to a switch port configured as hybrid mode     Hub Cluster:  3 Masters:  $BASE_DOMAIN: mgmt-hub.e2e.bos.redhat.com DHCP Range: 2620:52:0:1302::11,2620:52:0:1302::20,64 ingressVIP: apps.$BASE_DOMAIN: 2620:52:0:1302::2 apiVIP: api.$BASE_DOMAIN: 2620:52:0:1302::3 dnsVIP: ns1.$BASE_DOMAIN: 2620:52:0:1302::4 master0: openshift-master-0.$BASE_DOMAIN 2620:52:0:1302::5 master1: openshift-master-1.$BASE_DOMAIN 2620:52:0:1302::6 master2: openshift-master-2.$BASE_DOMAIN 2620:52:0:1302::7     Spoke1:  3 Masters:  $BASE_DOMAIN :mgmt-spoke1.e2e.bos.redhat.com DHCP Range …","date":-62135596800,"description":"","objectID":"4864a2b1a092076d60869d4695e28d85","permalink":"https://pages.sysdeseng.com/labs/deploy/lab-physical-ipv6disc/","tags":["baremetal","labs","IPv6","disconnected","physical"],"title":"Physical IPv6 Disconnected Lab"},{"content":"References:   Deploy Folder: https://github.com/RHsyseng/labs-index/tree/master/deploy-rhacm\n  quay.io RHACM tags: https://quay.io/repository/acm-d/acm-custom-registry?tab=tags\n  quay.io RHACM bundle: https://quay.io/repository/acm-d/acm-operator-bundle?tab=tags\n  official doc for deployment: https://github.com/open-cluster-management/deploy\n  The way to deploy: https://github.com/jparrill/ztp-the-hard-way\n  Requirements  Existing OCP cluster ready. Follow this instructions to get ready: https://github.com/RHsyseng/labs-index/blob/master/docs/lab-kcli-ipi.md LocalStorage deployed on the OCP cluster. Follow these instructions to get deployed: https://github.com/RHsyseng/labs-index/blob/master/docs/deploy-LocalStorage-operator.md. Remember if you\u0026amp;rsquo;re using a kcli plan to deploy IPI baremetal, NFS disks will be installed instead of LS. /etc/host entry in your local to get access to the multi-cloud website (multicloud-console.apps.\u0026amp;lt; clustername \u0026amp;gt;.\u0026amp;lt; domain \u0026amp;gt;):  192.168.122.253 api.alb-test-ci.alklabs.com console-openshift-console.apps.alb-test-ci.alklabs.com oauth-openshift.apps.alb-test-ci.alklabs.com prometheus-k8s-openshift-monitoring.apps.alb-test-ci.alklabs.com ocp-metal-ui-assisted-installer.apps.alb-test-ci.alklabs.com assisted-service-assisted-installer.apps.alb-test-ci.alklabs.com multicloud-console.apps.alb-test-ci.alklabs.com Install the RHACM upstream (only connected) This is the official way to deploy the release upstream. Pretty simple, just creating the next manifests:\nFirst thing is to create the namespace with oc apply -f 01-rhacm-ns.yml\napiVersion: v1 kind: Namespace metadata: labels: openshift.io/cluster-monitoring: \u0026amp;#34;true\u0026amp;#34; name: open-cluster-management Then you should create the operator group with oc apply -f 02-operatorGroup.yml\napiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: advanced-cluster-management-operatorgroup namespace: open-cluster-management spec: targetNamespaces: - open-cluster-management …","date":-62135596800,"description":"","objectID":"f955409a0f0f15b0b6923f7b9b0b403a","permalink":"https://pages.sysdeseng.com/labs/deploy/rhacm/","tags":["ACM","labs"],"title":"Red Hat Advanced Cluster Management (ACM)"},{"content":"References Lab-index branch: sno-acm-stark\nhttps://github.com/RHsyseng/labs-index/tree/master/lab-kcli-ipi-baremetal\nhttps://github.com/RHsyseng/labs-index/tree/master/deploy-rhacm\nPrevious Requirements  Single Node Cluster deployed and ready to be used. In the reference section, you will find information about the installation of a Single Node Cluster. This document begins with the installation of the ACM and the Spoke SNO on top of the Single Node Cluster.  ACM Installation Using the next script to install the ACM on the Single Node Cluster:\n#!/usr/bin/env bash  set -o errexit set -o pipefail set -o nounset set -m # variables # ######### export OC_CLUSTER_NAME=test-ci export OC_SNAPSHOT=2.3.0-DOWNSTREAM-XXXXXXXXX # replace with the snapshot you want to use  export OC_BUNDLE=v2.3.0-117 # replace with the bundle you want to use export HOST_INSTALLER=$host_installer # vm or host with the registry mirror if needed, dhcp and dns,  export IP_INSTALLER=$ip_installer # ip address for the installer host export SHARE_DIR=$shared_dir # lab-index/shared_folder export OC_AMORGANT_QUAY= # pull secret for quay  export OC_TYPE_ENV=connected # type of environment export OC_NET_CLASS=\u0026amp;#34;ipv4\u0026amp;#34; # network class source ../$SHARED_DIR/inventory.sh export KUBECONFIG=~/.kcli/clusters/$OC_CLUSTER_NAME/auth/kubeconfig echo \u0026amp;#34;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; Set the ip format depending on ipv4 / ipv6\u0026amp;#34; echo \u0026amp;#34;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;#34; export ip=\u0026amp;#34;\u0026amp;#34; if [ \u0026amp;#34;$OC_NET_CLASS\u0026amp;#34; = \u0026amp;#34;ipv4\u0026amp;#34; ]; then ip=\u0026amp;#34;$IP_INSTALLER\u0026amp;#34; else ip=\u0026amp;#34;[$IP_INSTALLER]\u0026amp;#34; fi echo \u0026amp;#34;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt; Copy files to installer node\u0026amp;#34; echo \u0026amp;#34;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;#34; scp ./start-prereq.sh root@\u0026amp;#34;$ip\u0026amp;#34;:~/ scp …","date":-62135596800,"description":"","objectID":"9d44b63241ede0559f9a7ad1769c4945","permalink":"https://pages.sysdeseng.com/labs/deploy/sno-acm-spokesno-scripts/","tags":["ACM","sno","scripts"],"title":"Deploy a Hub SNO with ACM and Spoke SNO on top of it using scripts"},{"content":" References: ZTP For Telco Edge 5G  What is ZTP? Context on ZTP architecture  Disconnected ZTP Flow Connected ZTP Flow ZTP Hub Components   How to start with ZTP?    References:  The original repository: https://github.com/jparrill/ztp-the-hard-way  ZTP For Telco Edge 5G Here, we will collect all the info around the ZTP flow, which are the preferred scenarios, the steps to follow and much more. Let\u0026amp;rsquo;s get started from the beginning with some context and theory.\nWhat is ZTP? ZTP stands for Zero Touch Provisioning, a project to deploy and deliver OpenShift 4 in a HUB-SPOKE architecture (in a relation of 1-N), where the HUB will manage many Spokes. The Hub and the Spokes will be based on OpenShift 4 but with the difference that the HUB cluster will manage and deploy the spokes using RHACM (Red Hat Advanced Cluster Management).\nWhy Zero Touch Provisioning If I need to deploy some things by hand?, well this is a fair question, we need to have a consistent base for the hub to perform the Spoke deployments and for that we need to deploy OpenShift Container Platform 4 (On an IPI way) if not we will have the egg-chicken issue.\nWhy is this related with Single Node OpenShift (SNO) and Remote Worker Node (RWN)?, in the 5G world exists some areas called RAN (Radio Access Network) here we have some scenarios but the important points here is, SNO will be mostly on the D-RAN places and eventually on C-RAN ones, this happens in the same way with RWN.\nContext on ZTP architecture On a high level view, we have two scenarios, the connected world and the disconnected world which means, that your OpenShift nodes can access directly to the Internet or not. From here we need to separate them in two ways to follow. the disconnected one will need to fill some pre-requisites before the action starts, let\u0026amp;rsquo;s take a look to some diagrams:\nDisconnected ZTP Flow Connected ZTP Flow These are the steps 1-by-1 that we need to follow in order to deploy every element of ZTP, including the Hub …","date":-62135596800,"description":"","objectID":"ca4df6ecc4035a61618150a4ef6d233d","permalink":"https://pages.sysdeseng.com/labs/deploy/ztp-the-hard-way/","tags":["ZTP","labs","IPv6","disconnected","physical"],"title":"ZTP The Hard Way"},{"content":"This folder contains the IPI examples\n","date":-62135596800,"description":"","objectID":"3bf6960f8124d2f1a2402cd94c690f2a","permalink":"https://pages.sysdeseng.com/labs/ocp/ipi/","title":"Installer Provisioned Infrastructure (IPI)"},{"content":"References:  Lab Folder: https://github.com/RHsyseng/labs-index/tree/master/lab-kcli-ipi-baremetal Paramfile (Inside the lab folder the paramfile to be used): lab-metal3.yml  AUTOMATIC installation with this three parameters, the installation will be automatic from scratch to finish:\n lab:true launch_steps: true deploy_openshift: true  Edit your lab-metal3.yml maybe you could copy from file .example in kcli plan:\nlab: true version: ci provisioning_enable: false virtual_protocol: redfish uefi_legacy: true virtual_masters: true virtual_workers: false launch_steps: true deploy_openshift: true cluster: test-ci domain: alklabs.com baremetal_cidr: 192.168.150.0/24 baremetal_net: bare-net provisioning_net: lab-prov virtual_masters_memory: 16000 virtual_masters_numcpus: 16 virtual_workers_deploy: false api_ip: 192.168.150.253 ingress_ip: 192.168.150.252 ipmi_user: amorgant ipmi_password: alknopfler baremetal_ips: - 192.168.150.10 - 192.168.150.11 - 192.168.150.12 baremetal_macs: - aa:aa:aa:ba:cc:11 - aa:aa:aa:ba:cc:22 - aa:aa:aa:ba:cc:33 Just to launch everything and wait until finish (without any manual interaction):\nkcli create plan --paramfile=lab-metal3.yml -P version=ci -P openshift_image=${{github.event.inputs.OC_RELEASE}} -P cluster=${{github.event.inputs.OC_CLUSTER_NAME}} ${{github.event.inputs.OC_CLUSTER_NAME}} MANUAL installation As you can see in the repository, you could change the flag:\n launch_steps: false  And the plan will be launched, the installation VM will be created and the rest of VMs will be off until you start to launch script by yourself. At this point will be manually deployed as a lab environment.\nNote maybe some steps will not be necessary.\n[root@qct-d14u09 lab-kcli-ipi-baremetal]# tree . |-- 00_virtual.sh.ipmi |-- 00_virtual.sh.redfish |-- 01_patch_installconfig.sh |-- 02_packages.sh |-- 03_network.sh |-- 04_get_clients.sh |-- 05_cache.sh |-- 06_disconnected.sh |-- 07_nbde.sh |-- 08_ntp.sh |-- 09_deploy_openshift.sh |-- 10_nfs.sh |-- 10_nfs.yml …","date":-62135596800,"description":"","objectID":"0304d6c31eda8e9d06b371218f03ac1e","permalink":"https://pages.sysdeseng.com/labs/ocp/ipi/kcli-ipi-metal3-ipv4con/","tags":["IPI","Metal³","labs","IPv4","kcli"],"title":"kcli with Metal³ and IPv4 connected"},{"content":"References:  Lab Folder: https://github.com/RHsyseng/labs-index/tree/master/lab-kcli-ipi-baremetal Paramfile (Inside the lab folder the paramfile to be used): lab_ipv6.yml  Requirements: This documents, starts from an installation ready with IPI Baremetal 4.8 - Disconnected / IPv6 but using virtual VM to simulate a real scenario.\nIn our case, we don\u0026amp;rsquo;t need the lab mode so we\u0026amp;rsquo;re just installing using the next lab.yml.\nAUTOMATIC installation With this three parameters, the installation will be automatic from scratch to finish:\n lab: true launch_steps: true deploy_openshift: true  Edit your lab_ipv6.yml lab: true provisioning_enable: false pool: default disconnected: true virtual_masters: true launch_steps: true deploy_openshift: true version: ci tag: 4.8.0-0.nightly-2021-06-01-014458 openshift_version: 4.8 cluster: lab domain: alklabs.com openshift_image: registry.ci.openshift.org/ocp/release:4.8.0-0.nightly-2021-06-01-014458 baremetal_cidr: 2620:52:0:1302::/64 baremetal_net: lab-net provisioning_net: lab-prov virtual_masters_memory: 16384 virtual_masters_numcpus: 8 virtual_workers_deploy: false api_ip: 2620:52:0:1302::2 ingress_ip: 2620:52:0:1302::3 disk_size: 300 ipmi_user: jimi ipmi_password: hendrix disconnected_operators: - advanced-cluster-management - local-storage-operator - ocs-operator - performance-addon-operator - ptp-operator - sriov-network-operator baremetal_ips: - 2620:52:0:1302::20 - 2620:52:0:1302::21 - 2620:52:0:1302::22 baremetal_macs: - aa:aa:aa:aa:bb:01 - aa:aa:aa:aa:bb:02 - aa:aa:aa:aa:bb:03 The OCP release will be mirrored to this environment:\nversion: ci tag: 4.8.0-0.nightly-2021-06-01-014458 openshift_version: 4.8 cluster: lab domain: alklabs.com openshift_image: registry.ci.openshift.org/ocp/release:4.8.0-0.nightly-2021-06-01-014458 For sure, the most important is that automatically with the plan the release OCP will be mirrored\nAs you can see, you can also specify several disconnected operators which will be mirrored from OLM …","date":-62135596800,"description":"","objectID":"4d3e56185be76b97659262e709475eca","permalink":"https://pages.sysdeseng.com/labs/ocp/ipi/kcli-ipi-metal3-ipv6disc/","tags":["IPI","Metal³","labs","IPv6","kcli"],"title":"kcli with Metal³ and IPv6 disconnected"},{"content":"References:  Lab Folder: https://github.com/RHsyseng/labs-index/tree/master/lab-kcli-ipi-baremetal Paramfile (Inside the lab folder the paramfile to be used): lab-withoutMetal3.yml  Prepare the environment OCP previous installation: We\u0026amp;rsquo;re going to deploy an IPI cluster (all versions, but for the example, we will use 4.8) using kcli to deploy ASAP.\nIn this example, we\u0026amp;rsquo;ve got two kinds of deployment:\n SNO Multinode  The first one is the multi-node. To deploy the multi-node OCP cluster:\n Multi-node Parameters.yml:  cluster: qct11 master_memory: 32768 domain: karmalabs.com version: ci tag: 4.8.0-0.ci-2021-03-24-043821 numcpus: 12 bootstrap_memory: 8192 masters: 3 api_ip: 192.168.122.253 apps: - users  Single Node Parameters.yml  cluster: ocp sno: true sno_virtual: true master_memory: 32768 domain: e2e.bos.redhat.com version: ci tag: 4.8.0-0.nightly-2021-04-18-203506 numcpus: 8 bootstrap_memory: 8192 masters: 1 apps: - users Pull secret: ln -s pull-secret.yaml openshift_pull.json Pull secret must have registry.ci line if you\u0026amp;rsquo;re deploying a non-stable version\nNOTE: The release_image variable could change depending on the date you are executing this, just verify here which is the most suitable for you, and remember that you need to perform the mirror sync.\nSet the DNS or /etc/hosts: 192.168.122.253 api.qct11.karmalabs.com console-openshift-console.apps.qct11.karmalabs.com oauth-openshift.apps.qct11.karmalabs.com prometheus-k8s-openshift-monitoring.apps.qct11.karmalabs.com ocp-metal-ui-assisted-installer.apps.qct11.karmalabs.com assisted-service-assisted-installer.apps.qct11.karmalabs.com Deploy kcli cluster kcli create plan --paramfile lab-withoutMetal3.yml test-ci  Special steps for the SNO deployment\nAccessing SNO install HAProxy dnf install haproxy -y configure /etc/haproxy/haproxy.cfg global log 127.0.0.1 local2 maxconn 4000 daemon defaults mode tcp log global #option httplog #option dontlognull #option http-server-close #option forwardfor except …","date":-62135596800,"description":"","objectID":"dfa41374e12e1f17abdc17834c3339c4","permalink":"https://pages.sysdeseng.com/labs/ocp/ipi/kcli-ipi-withoutmetal3-ipv4con/","tags":["IPI","labs","IPv4","kcli"],"title":"kcli without Metal³ and IPv4 connected"},{"content":"reference:\nLab Folder: N/A (There is not Lab folder created for this one because we\u0026amp;rsquo;re following the official cloud doc to get the binary and installing directly)\nURL guides followed:  GCP: https://www.openshift.com/blog/ocp-4.6-install-on-gcp-cloud-the-smooth-experience AWS: https://docs.openshift.com/container-platform/4.7/installing/installing_aws/installing-aws-default.html  Download IPI installer. There are two ways:\n Mirror: https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable/ Official site: https://cloud.redhat.com/openshift/create/cloud and https://cloud.redhat.com/openshift/install/aws  Procedure:\n Download the IPI installer from the web. Download the pull secret Install oc client in your path if you didn’t install it before  Configure DNS zone When IPI installer is run, one of the interactive steps looks for DNS name to use in the cluster. This search is performed in Route53 within the account that is used with the credentials provided. Hence it is required to create a DNS zone to use in the installation. Below are the steps required to create this zone.\n Create subdomain Zone: Open the AWS Console and go to Services \u0026amp;gt; Route53 \u0026amp;gt; Hosted zones. In there, click on Create hosted zone and follow the form to create your lab domain. For this example we are going to use mylab.sysdeseng.com domain name. Click on Create zone and, once the zone has been created, copy the name of the NS servers for your new zone, will be required later.  Add subdomain NS entry to main zone: As the zone created is a subdomain from the main zone sysdeseng.com, it is required to add a NS entry in the main zone to point to the DNS servers of the new subdomain zone. Just open the sysdeseng.com zone in the Route53 console and click on Create record. In the opened form fill in with the following values: _ Record name: mylab _ Record type: NS - Name servers for a hosted zone _ Value: (paste your subdomain zone DNS server list) _ TTL: 300 Click on Create records when …","date":-62135596800,"description":"","objectID":"b429f98704b2fff65d24181bf6d213ea","permalink":"https://pages.sysdeseng.com/labs/ocp/ipi/openshift-4.7-aws/","tags":["OpenShift","IPI","labs","AWS"],"title":"OpenShift 4.7 on AWS"},{"content":"This folder contains the UPI examples\n","date":-62135596800,"description":"","objectID":"b100baa571e7b9a27a1621db5bd4a76b","permalink":"https://pages.sysdeseng.com/labs/ocp/upi/","title":"User Provisioned Infrastructure (UPI)"},{"content":"Reference Lab Folder: https://github.com/RHsyseng/labs-index/tree/master/lab-manual-sno-upi-baremetal\nExternal ref: https://github.com/eranco74/bootstrap-in-place-poc\nFirst of all, download the rhcos-live image curl -Lk https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/latest/latest/rhcos-live.x86_64.iso \u0026amp;gt; /var/lib/libvirt/images/rhcos-live.x86_64.iso Create an install config (take a look at the “samples” folder in lab-manual-sno-upi-baremetal) You need to add the SSH pub key and the pull secret to your install-config.yaml.\n# Set to 0 the number of workers compute.replicas: 0 # Set to 1 the number of masters controlPlane.replicas: 1 # Set platform to none platform.none:{} # IMPORTANT! ( If you’re using physical baremetal /dev/sda) bootstrapInPlace.InstallationDisk: /dev/vda Create the ignition files for the SNO deployment Install the OpenShift installer with the command below (be careful setting the proper version in the URL).\n[root@qct-d14u11 sno-baremetal]# curl https://mirror.openshift.com/pub/openshift-v4/amd64/clients/ocp/4.8.0/openshift-install-linux-4.8.0.tar.gz -o openshift-install-linux-4.8.0.tar.gz Launch the command below to create the ignition file.\n[root@qct-d14u11 sno-baremetal]# openshift-install create single-node-ignition-config INFO Consuming Install Config from target directory WARNING Making control-plane schedulable by setting MastersSchedulable to true for Scheduler cluster settings INFO Single-Node-Ignition-Config created in: . and auth Create the embedded ISO image with the ignition correctly generated before: [root@qct-d14u11 sno-baremetal]# cp /var/lib/libvirt/images/rhcos-live.x86_64.iso . [root@qct-d14u11 sno-baremetal]# coreos-installer iso ignition embed ./rhcos-live.x86_64.iso -i config.ign -f You need to copy to the libvirt route\n[root@qct-d14u11 sno-baremetal]# cp rhcos-live.x86_64.iso /var/lib/libvirt/images/rhcos-embed-sno.x86_64.iso Create VM with the ISO attached [root@qct-d14u11 sno-baremetal]# kcli create vm …","date":-62135596800,"description":"","objectID":"c689a3a81eeb35614974a177bdf23987","permalink":"https://pages.sysdeseng.com/labs/ocp/upi/sno-upi-baremetal/","tags":["SNO","UPI","labs","IPv4","baremetal"],"title":"Manual SNO on baremetal"},{"content":"Disclaimer \u0026amp;nbsp;UNSUPPORTED This repository and its contents are completely UNSUPPORTED in any way and are not part of official documentation.   \u0026amp;nbsp;Contribute! The documents hosted here are just personal notes, if you feel that you can contribute into polishing them as guides, feel free to let us know via issues to this repository:\nhttps://gihub.com/RHsyseng/labs\n  Description These are the folders where the labs to install OCP in different ways (IPI, UPI, IPv6, and so on) are located.\nJust only with these folders should be enough to deploy an OpenShift cluster\nIs possible we could use the same lab-* folder for different kind of deployments, just changing some files and parameters.\nTo do that in the docs folder should be one doc for each kind of deployment explaining the way to reuse for other kind of deployments:\n For instance, lab-kcli-ipi-baremetal, could be used to deploy:  IPI Baremetal + Metal³ + connected IPv4 IPI Baremetal + without Metal³ + connected IPv4 IPI Baremetal + Metal³ + disconnected IPv6    ","date":-62135596800,"description":"","objectID":"dcc8c155e9a13ac9a9675b72d833234b","permalink":"https://pages.sysdeseng.com/labs/ocp/","title":"Labs to install an OpenShift cluster"},{"content":"Disclaimer \u0026amp;nbsp;UNSUPPORTED This repository and its contents are completely UNSUPPORTED in any way and are not part of official documentation.   \u0026amp;nbsp;Contribute! The documents hosted here are just personal notes, if you feel that you can contribute into polishing them as guides, feel free to let us know via issues to this repository:\nhttps://gihub.com/RHsyseng/labs\n  Description This is a go library created to validate a pull-secret, keeping in mind the next concepts:\n Inputs: [] byte (with the pull secret in json format) Output: Struct of interfaces with the information structured in 3 parts:  Valid Entries Expired Entries Connection Issues (Sometimes could be an internal registry with some problems to be connected)    How to use it There are two repositories that consume the library. One of them is for the WebUI deployed just for internal staff engineering:\n pullsecret-validator repository https://github.com/RHsyseng/pullsecret-validator WebUI is deployed here (just for internal staff): http://pullsecret-validator-pullsecret-validator.apps.shift.cloud.lab.eng.bos.redhat.com/  And the second one is a CLI to use in your VM, host, instance, even inside your CI/CD pipeline:\n pullsecret-validator-cli repository https://github.com/RHsyseng/pullsecret-validator-cli The binaries for most platform: https://github.com/RHsyseng/pullsecret-validator-cli/releases/latest  How to use it (as a developer) Just import or get the package using this command:\ngo get github.com/RHsyseng/lib-ps-validator import github.com/RHsyseng/lib-ps-validator Main function is:\nfunc Validate(input []byte) WebData Where WebData is the model:\ntype WebData struct { Input interface{} ResultOK interface{} ResultKO interface{} ResultCon interface{} } ","date":-62135596800,"description":"","objectID":"3370d94183519802d5fe06d7914a1bea","permalink":"https://pages.sysdeseng.com/labs/ocp/pull-secret-validator/","title":"Pull Secret Validator tool"},{"content":" ACM: Advanced Cluster Manager. Allows to manage clusters in a structure of one-to-many (Hub-Spoke) for orchestration. HUB: Cluster executing ACM with one or more Spoke clusters assigned. IPI: Installer-Provisioned Infrastructure NFV: Network Functions Virtualization OCP: OpenShift Container Platform OKD: OpenShift Kubernetes Distribution is the upstream for the OpenShift Container Platform product. OpenShift: OpenShift is a family of containerization software products developed by Red Hat. Its flagship product is the OpenShift Container Platform — an on-premises platform as a service built around Docker containers orchestrated and managed by Kubernetes on a foundation of Red Hat Enterprise Linux. The family\u0026amp;rsquo;s other products provide this platform through different environments: OKD serves as the community-driven upstream (akin to the way that Fedora is upstream of Red Hat Enterprise Linux), OpenShift Online is the platform offered as software as a service, and OpenShift Dedicated is the platform offered as a managed service. (From Wikipedia) PTP: Precision Time Protocol RAN: Radio Access Networks RWN: Remote Worker Nodes SDN: Software-Defined Networking SNO: Single Node OpenShift SNO: Single Node OpenShift, compared to the traditional 3-master nodes minimal setup, SNO allows OpenShift to be installed on a single node that better suites Telco 5G needs. Spoke: Cluster depending of HUB UPI: User-Provisioned Infrastructure ZTP: Zero Touch Provisioning  ","date":-62135596800,"description":"","objectID":"1547b81c9cf59c410d049e8641f790da","permalink":"https://pages.sysdeseng.com/labs/vocabulary/vocabulary/","tags":["keywords","vocabulary"],"title":"Vocabulary"},{"content":"Disclaimer \u0026amp;nbsp;UNSUPPORTED This repository and its contents are completely UNSUPPORTED in any way and are not part of official documentation.   \u0026amp;nbsp;Contribute! The documents hosted here are just personal notes, if you feel that you can contribute into polishing them as guides, feel free to let us know via issues to this repository:\nhttps://gihub.com/RHsyseng/labs\n  Description The idea of this repository is to offer everything necessary to automate the deployment processes of OpenShift laboratories and operators, as well as to document those deployments.\nThe main advantages are:\n Documentation organized and versioned that serves as a ramp-up for new teammates. Automation of repetitive tasks such as deploying basic operators necessary for all tasks we have to do. Avoid wasting time deploying the same OpenShift environment over and over again. You can prepare the environment the night before to start working next day with a clean and ready environment. These deployments use \u0026amp;ldquo;runners\u0026amp;rdquo; (small software agents installed on the lab hosts) which will execute the workflow steps directly on the lab hosts. Runners can be added simply by installing an agent. The host must be installed with OS in order to be used as a runner. Moreover, libvirtd should be installed for most of the labs in order to create VM on the host.  Repository Structure Basically, there are four kinds of folders and things here:\nHow does the workflows work Install the action_runner in your target host The agent has a pulling architecture, so you don\u0026amp;rsquo;t need anything about VPN or whatever\nFollow these instructions\nThe idea is to configure a tag during the action runner with your name (for instance amorgant) to use after in our workflow as a host to run it. To do that in the YML workflow file you must specify the tag that you created during the runner installation\nAn example to automate just changing some parameters:\nRUNNER_ALLOW_RUNASROOT=\u0026amp;#34;1\u0026amp;#34; ./config.sh --url \u0026amp;lt;repo_url\u0026amp;gt; …","date":-62135596800,"description":"","objectID":"3976528693a0108357f4928017600865","permalink":"https://pages.sysdeseng.com/labs/","tags":["labs"],"title":"Labs index"},{"content":"       async function startLunrJSAsync() { console.log(\u0026#34;search: Starting Lunr...\u0026#34;); let ok = false; const lunrIndex = \u0026#34;https:\\/\\/pages.sysdeseng.com\\/labs/algolia.json\u0026#34;; const lunrSummary = \u0026#34;https:\\/\\/pages.sysdeseng.com\\/labs/algolia.json\u0026#34;; console.log(\u0026#34;search: Fetching Index...\u0026#34;); let response = await fetch(lunrIndex); let data = await response.json(); var idx = lunr(function () { this.field(\u0026#39;id\u0026#39;); this.field(\u0026#39;title\u0026#39;, { boost: 3 }); this.field(\u0026#39;date\u0026#39;); this.field(\u0026#39;content\u0026#39;); this.field(\u0026#39;permalink\u0026#39;); for (var key in data) { this.add({ \u0026#39;id\u0026#39;: key, \u0026#39;title\u0026#39;: data[key].title, \u0026#39;date\u0026#39;: data[key].date, \u0026#39;content\u0026#39;: data[key].content, \u0026#39;permalink\u0026#39;: data[key].permalink, }); } }); pages = data console.log(\u0026#34;search: Index Loaded!\u0026#34;); console.log(\u0026#34;search: Lunr Is Ready!\u0026#34;); ok = true; let obj = { idx: idx, pages: pages, ok: ok }; return obj; } function searchSite(search, query) { let template = document.querySelector(\u0026#34;#search-item\u0026#34;); let resultsContainer = document.querySelector(\u0026#34;#search-results\u0026#34;); resultsContainer.innerHTML = \u0026#34;\u0026#34;; let allResults = search.idx.search(query); if (allResults.length === 0) resultsContainer.innerHTML = \u0026#34;Nothing found; search for something else!\n\u0026#34;; else allResults.forEach(function (result) { let output = document.importNode(template.content, true); let title = output.querySelector(\u0026#34;a\u0026#34;); let breadcrumb = output.querySelector(\u0026#34;aside\u0026#34;); let summary = output.querySelector(\u0026#34;p\u0026#34;); let docRef, typemoji; title.innerHTML = pages[result.ref].title; title.setAttribute(\u0026#34;href\u0026#34;, pages[result.ref].permalink); breadcrumb.innerHTML = pages[result.ref].title; resultsContainer.appendChild(output); }); } (async () = { let Search = await startLunrJSAsync(); let query; let params = new URLSearchParams(document.location.search.substring(1)); if (params.get(\u0026#34;q\u0026#34;)) { document.getElementById(\u0026#34;search-input\u0026#34;).value = params.get(\u0026#34;q\u0026#34;); query = params.get(\u0026#34;q\u0026#34;); searchSite(Search, query); } })();   Search site      \n   ","date":-62135596800,"description":"","objectID":"4953bf99d7018c161b1b361cb25f1331","permalink":"https://pages.sysdeseng.com/labs/search/","title":"Search"}]